{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3fa336e0886348258e5e098c6de70d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85ee9243b7c944bfb4eec62e028263d8",
              "IPY_MODEL_50dafe851b6444e198e602c043ed54a9",
              "IPY_MODEL_11e2d504bf1d4438961ec8bcb8937ad8"
            ],
            "layout": "IPY_MODEL_58644667d1154dce97be68513bf61372"
          }
        },
        "85ee9243b7c944bfb4eec62e028263d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_220237270fa349a09e541a754163c370",
            "placeholder": "​",
            "style": "IPY_MODEL_f9d47bc9c26144b3b0001d55a4fec037",
            "value": "Dl Completed...: 100%"
          }
        },
        "50dafe851b6444e198e602c043ed54a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_738c3a05ba8744c79b4e41611f3831ef",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06ae861fa51644288441f7b3b883c52d",
            "value": 1
          }
        },
        "11e2d504bf1d4438961ec8bcb8937ad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bee1c76d423f4c56a4d76ab11a40fa10",
            "placeholder": "​",
            "style": "IPY_MODEL_7b0c392e641e4e6fa5de093e14b9349b",
            "value": " 2/2 [01:36&lt;00:00, 38.36s/ url]"
          }
        },
        "58644667d1154dce97be68513bf61372": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "220237270fa349a09e541a754163c370": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9d47bc9c26144b3b0001d55a4fec037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "738c3a05ba8744c79b4e41611f3831ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "06ae861fa51644288441f7b3b883c52d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bee1c76d423f4c56a4d76ab11a40fa10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b0c392e641e4e6fa5de093e14b9349b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f882662a3b44bfeb5892c1dad9aad98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8f6a4305e89477d8875c92f5ffc7285",
              "IPY_MODEL_43ce9ca11c5a4a89a2ab7493ce921998",
              "IPY_MODEL_697a8e21ba994881b5e4d109178406dc"
            ],
            "layout": "IPY_MODEL_0d3b3fe380e54b548b25e4faadfa69f7"
          }
        },
        "a8f6a4305e89477d8875c92f5ffc7285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0edc81d8bb249529b301c9b0c6990df",
            "placeholder": "​",
            "style": "IPY_MODEL_70cf43ae820b443bb76a14c86dafd278",
            "value": "Dl Size...: 100%"
          }
        },
        "43ce9ca11c5a4a89a2ab7493ce921998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1048968bcd474c6ebecb8795f6e127bd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_454d4aa3b68042399285c3af086bb1a1",
            "value": 1
          }
        },
        "697a8e21ba994881b5e4d109178406dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d5c65d1e4c6456ba493d9ff3758d791",
            "placeholder": "​",
            "style": "IPY_MODEL_aecc28ddc8b946ef8d85c25db95fa18b",
            "value": " 773/773 [01:36&lt;00:00, 13.05 MiB/s]"
          }
        },
        "0d3b3fe380e54b548b25e4faadfa69f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0edc81d8bb249529b301c9b0c6990df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70cf43ae820b443bb76a14c86dafd278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1048968bcd474c6ebecb8795f6e127bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "454d4aa3b68042399285c3af086bb1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d5c65d1e4c6456ba493d9ff3758d791": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aecc28ddc8b946ef8d85c25db95fa18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c63ff2f8fe794c318e5439934cf182f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c22660f1a0d4b528e01924589b9640b",
              "IPY_MODEL_c1ca7f640a884656a3c2e58feaa46ad8",
              "IPY_MODEL_755731e323be4b46bd1e75dd15b18743"
            ],
            "layout": "IPY_MODEL_1114ff13d0e84383a5da7abb20f2fda6"
          }
        },
        "6c22660f1a0d4b528e01924589b9640b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e062366958a44d82b1a230727617b0e7",
            "placeholder": "​",
            "style": "IPY_MODEL_37ddf6ea954a4406ad2ff8b654a6370f",
            "value": "Extraction completed...: 100%"
          }
        },
        "c1ca7f640a884656a3c2e58feaa46ad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b380a8cc8e4a4f42880a46c7470f2f8b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c928de06544a4ce182e4f94f5bccefdc",
            "value": 0
          }
        },
        "755731e323be4b46bd1e75dd15b18743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c81989344544b9cb73552506b35a69d",
            "placeholder": "​",
            "style": "IPY_MODEL_22d21ec3e9e44c1ea6b3057dba6d55a3",
            "value": " 18473/18473 [01:36&lt;00:00, 515.74 file/s]"
          }
        },
        "1114ff13d0e84383a5da7abb20f2fda6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e062366958a44d82b1a230727617b0e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37ddf6ea954a4406ad2ff8b654a6370f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b380a8cc8e4a4f42880a46c7470f2f8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c928de06544a4ce182e4f94f5bccefdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c81989344544b9cb73552506b35a69d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22d21ec3e9e44c1ea6b3057dba6d55a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuvayan/AIMLOPS/blob/main/Copy_of_M2_AST_05_Image_Segmentation_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A programme by IISc and TalentSprint\n",
        "### Assignment 5: Image Segmentation using U-Net and DeepLabv3+"
      ],
      "metadata": {
        "id": "zSstSEIv155v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "*  understand, prepare, and visualize the the dataset containing image and corresponding masked image used for segmentation\n",
        "*  understand the encoder, bottleneck, and decoder region of a U-Net architecture\n",
        "*  build and train a U-Net architecture for segmentation\n",
        "*  create a masked image (prediction)\n",
        "*  calculate the accuracy score like IoU and Dice-Score used in segmentation\n",
        "* understand and implement DeeplabV3+ architecture for segmentation"
      ],
      "metadata": {
        "id": "akVZjBjD2PtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "We will be training the model on the [Oxford Pets - IIIT](https://www.robots.ox.ac.uk/~vgg/data/pets/) dataset. This contains pet images, their classes, segmentation masks, and head region of interest. We will only use the images and segmentation masks for this experiment.\n",
        "\n",
        "The dataset consists of images of 37 pet breeds, with 200 images per breed (~100 each in the training and test splits). Each image includes the corresponding label and pixel-wise masks. The masks are class labels for each pixel. Each pixel is given one of three categories:\n",
        "\n",
        "* Class 1: Pixel belonging to the pet.\n",
        "* Class 2: Pixel bordering the pet.\n",
        "* Class 3: None of the above/a surrounding pixel."
      ],
      "metadata": {
        "id": "oLUH84qfbqIL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df6l7a1oI8O7"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2303646\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk7BcPZsI8PF"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"9002239227\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3K_U4pCrI8PF",
        "outputId": "231f0182-a2f8-4019-a7cd-5bb56d653ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2303646&recordId=4856\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M2_AST_05_Image_Segmentation_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import required packages"
      ],
      "metadata": {
        "id": "Q8AXIPH64g_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yhxJY5ubMo2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is already included in TensorFlow Datasets and you can simply download it. The segmentation masks are included in versions 3 and above. The cell below will download the dataset and place the results in a dictionary named dataset. It will also collect information about the dataset and we'll assign it to a variable named info. [Reference](https://www.tensorflow.org/tutorials/images/segmentation)"
      ],
      "metadata": {
        "id": "7tiDorA4vx67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset and get info\n",
        "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"
      ],
      "metadata": {
        "id": "WhsCcE6DcdiJ",
        "outputId": "72f1ef62-e933-4035-edd2-6203d1a7edeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "3fa336e0886348258e5e098c6de70d14",
            "85ee9243b7c944bfb4eec62e028263d8",
            "50dafe851b6444e198e602c043ed54a9",
            "11e2d504bf1d4438961ec8bcb8937ad8",
            "58644667d1154dce97be68513bf61372",
            "220237270fa349a09e541a754163c370",
            "f9d47bc9c26144b3b0001d55a4fec037",
            "738c3a05ba8744c79b4e41611f3831ef",
            "06ae861fa51644288441f7b3b883c52d",
            "bee1c76d423f4c56a4d76ab11a40fa10",
            "7b0c392e641e4e6fa5de093e14b9349b",
            "2f882662a3b44bfeb5892c1dad9aad98",
            "a8f6a4305e89477d8875c92f5ffc7285",
            "43ce9ca11c5a4a89a2ab7493ce921998",
            "697a8e21ba994881b5e4d109178406dc",
            "0d3b3fe380e54b548b25e4faadfa69f7",
            "a0edc81d8bb249529b301c9b0c6990df",
            "70cf43ae820b443bb76a14c86dafd278",
            "1048968bcd474c6ebecb8795f6e127bd",
            "454d4aa3b68042399285c3af086bb1a1",
            "7d5c65d1e4c6456ba493d9ff3758d791",
            "aecc28ddc8b946ef8d85c25db95fa18b",
            "c63ff2f8fe794c318e5439934cf182f8",
            "6c22660f1a0d4b528e01924589b9640b",
            "c1ca7f640a884656a3c2e58feaa46ad8",
            "755731e323be4b46bd1e75dd15b18743",
            "1114ff13d0e84383a5da7abb20f2fda6",
            "e062366958a44d82b1a230727617b0e7",
            "37ddf6ea954a4406ad2ff8b654a6370f",
            "b380a8cc8e4a4f42880a46c7470f2f8b",
            "c928de06544a4ce182e4f94f5bccefdc",
            "6c81989344544b9cb73552506b35a69d",
            "22d21ec3e9e44c1ea6b3057dba6d55a3"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 773.52 MiB (download: 773.52 MiB, generated: 774.69 MiB, total: 1.51 GiB) to /root/tensorflow_datasets/oxford_iiit_pet/3.2.0...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fa336e0886348258e5e098c6de70d14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f882662a3b44bfeb5892c1dad9aad98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extraction completed...: 0 file [00:00, ? file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c63ff2f8fe794c318e5439934cf182f8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the downloaded dataset"
      ],
      "metadata": {
        "id": "LPI24FDHcqiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(dataset), type(info))"
      ],
      "metadata": {
        "id": "n5G8ENsCuFUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see the possible keys we can access in the dataset dict.\n",
        "# this contains the test and train splits.\n",
        "print(dataset.keys())\n",
        "# see information about the dataset\n",
        "print(info)"
      ],
      "metadata": {
        "id": "uQdstUFwvULl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info.splits['test'].num_examples"
      ],
      "metadata": {
        "id": "gaHFXRXHt9nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info.splits['train'].num_examples"
      ],
      "metadata": {
        "id": "bjGMrlxUucdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the Dataset\n",
        "We will now prepare the train and test sets. The following utility functions preprocess the data. These include:\n",
        "\n",
        "* simple augmentation by flipping the image\n",
        "* normalizing the pixel values\n",
        "* resizing the images\n",
        "\n",
        "Another preprocessing step is to adjust the segmentation mask's pixel values. The README in the annotations folder of the dataset mentions that the pixels in the segmentation mask are labeled as such > {Class : Label} --> { 'foreground' : 1, 'background' : 2 , 'Not Classified' : 3 }.\n",
        "\n",
        "The image color values are normalized to the [0, 1] range. Finally, as mentioned above the pixels in the segmentation mask are labeled either {1, 2, 3}. For the sake of convenience, subtract 1 from the segmentation mask, resulting in labels that are : {0, 1, 2} and we will interpret these as {'pet', 'background', 'outline'}:"
      ],
      "metadata": {
        "id": "UP9xPeQReG7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Utilities\n",
        "\n",
        "def random_flip(input_image, input_mask):\n",
        "  '''does a random flip of the image and mask'''\n",
        "  if tf.random.uniform(()) > 0.5:\n",
        "    input_image = tf.image.flip_left_right(input_image)\n",
        "    input_mask = tf.image.flip_left_right(input_mask)\n",
        "\n",
        "  return input_image, input_mask\n",
        "\n",
        "\n",
        "def normalize(input_image, input_mask):\n",
        "  '''\n",
        "  normalizes the input image pixel values to be from [0,1].\n",
        "  subtracts 1 from the mask labels to have a range from [0,2]\n",
        "  '''\n",
        "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "  input_mask -= 1\n",
        "  return input_image, input_mask\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def load_image_train(datapoint):\n",
        "  '''resizes, normalizes, and flips the training data'''\n",
        "  input_image = tf.image.resize(datapoint['image'], (128, 128), method='nearest')\n",
        "  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128), method='nearest')\n",
        "  input_image, input_mask = random_flip(input_image, input_mask)\n",
        "  input_image, input_mask = normalize(input_image, input_mask)\n",
        "\n",
        "  return input_image, input_mask\n",
        "\n",
        "\n",
        "def load_image_test(datapoint):\n",
        "  '''resizes and normalizes the test data'''\n",
        "  input_image = tf.image.resize(datapoint['image'], (128, 128), method='nearest')\n",
        "  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128), method='nearest')\n",
        "  input_image, input_mask = normalize(input_image, input_mask)\n",
        "\n",
        "  return input_image, input_mask"
      ],
      "metadata": {
        "id": "yLQifV6Sgxvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now call the utility functions above to prepare the train and test sets. The dataset you downloaded from TFDS already contains these splits and you will use those by simpling accessing the train and test keys of the dataset dictionary.\n",
        "\n",
        "Note: The tf.data.experimental.AUTOTUNE you see in this notebook is simply a constant equal to -1. This value is passed to allow certain methods to automatically set parameters based on available resources. For instance, num_parallel_calls parameter below will be set dynamically based on the available CPUs. The docstrings will show if a parameter can be autotuned. Here is the entry describing what it does to num_parallel_calls."
      ],
      "metadata": {
        "id": "T1G-uxQOhCLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the train and test sets\n",
        "train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "test = dataset['test'].map(load_image_test)"
      ],
      "metadata": {
        "id": "UZ2V-eTehMG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize"
      ],
      "metadata": {
        "id": "0v3iYdbjfPY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for image, mask in train.take(10):\n",
        "  sample_image, sample_mask = image, mask"
      ],
      "metadata": {
        "id": "dFCrmctuirwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "img_arr = tf.keras.preprocessing.image.array_to_img(sample_image)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(img_arr)\n",
        "\n",
        "img_arr = tf.keras.preprocessing.image.array_to_img(sample_mask)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"True Mask\")\n",
        "plt.imshow(img_arr)\n"
      ],
      "metadata": {
        "id": "4jq1qZi1jS5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the splits are loaded, you can then prepare batches for training and testing."
      ],
      "metadata": {
        "id": "bI9jyr1Bhnrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "# shuffle and group the train set into batches\n",
        "train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "# do a prefetch to optimize processing\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# group the test set into batches\n",
        "test_dataset = test.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "Z89DI3k3hoZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizations"
      ],
      "metadata": {
        "id": "T4fHVQ7seQ25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_batch = next(iter(test_dataset))\n",
        "random_index = np.random.choice(sample_batch[0].shape[0])\n",
        "sample_image, sample_mask = sample_batch[0][random_index], sample_batch[1][random_index]"
      ],
      "metadata": {
        "id": "JksS78R5d8b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "img_arr = tf.keras.preprocessing.image.array_to_img(sample_image)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(img_arr)\n",
        "\n",
        "img_arr = tf.keras.preprocessing.image.array_to_img(sample_mask)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"True Mask\")\n",
        "plt.imshow(img_arr)"
      ],
      "metadata": {
        "id": "pY8kyw2teQkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implement the U-Net model**\n",
        "Define model : With the dataset prepared, we can now build the UNet. Here is the overall architecture as shown below. A U-Net consists of an encoder (downsampler) and decoder (upsampler) with a bottleneck in between. The gray arrows correspond to the skip connections that concatenate encoder block outputs to each stage of the decoder. Let's see how to implement these starting with the encoder.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/U-Net.png\" width=800px height=500px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "9I-abjYWiCo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder\n",
        "The encoder is having repeating blocks. It's best to create functions for it to make the code modular. These encoder blocks contain two Conv2D layers activated by ReLU, followed by a MaxPooling and Dropout layer. Each stage has an increasing number of filters and the dimensionality of the features reduce because of the pooling layer."
      ],
      "metadata": {
        "id": "a6THXDFOlm7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Encoder utilities with the following three functions:\n",
        "\n",
        "* conv2d_block() - to add two convolution layers and ReLU activations\n",
        "* encoder_block() - to add pooling and dropout to the conv2d blocks. Recall that in UNet, you need to save the output of the convolution layers at each block so this function will return two values to take that into account (i.e. output of the conv block and the dropout)\n",
        "* encoder() - to build the entire encoder. This will return the output of the last encoder block as well as the output of the previous conv blocks. These will be concatenated to the decoder blocks as you'll see later."
      ],
      "metadata": {
        "id": "_Q4bcXVwlqPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder Utilities\n",
        "\n",
        "def conv2d_block(input_tensor, n_filters, kernel_size = 3):\n",
        "  '''\n",
        "  Adds 2 convolutional layers with the parameters passed to it\n",
        "\n",
        "  Args:\n",
        "    input_tensor (tensor) -- the input tensor\n",
        "    n_filters (int) -- number of filters\n",
        "    kernel_size (int) -- kernel size for the convolution\n",
        "\n",
        "  Returns:\n",
        "    tensor of output features\n",
        "  '''\n",
        "  # first layer\n",
        "  x = input_tensor\n",
        "  for i in range(2):\n",
        "    x = tf.keras.layers.Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
        "            kernel_initializer = 'he_normal', padding = 'same')(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "def encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3):\n",
        "  '''\n",
        "  Adds two convolutional blocks and then perform down sampling on output of convolutions.\n",
        "\n",
        "  Args:\n",
        "    input_tensor (tensor) -- the input tensor\n",
        "    n_filters (int) -- number of filters\n",
        "    kernel_size (int) -- kernel size for the convolution\n",
        "\n",
        "  Returns:\n",
        "    f - the output features of the convolution block\n",
        "    p - the maxpooled features with dropout\n",
        "  '''\n",
        "\n",
        "  f = conv2d_block(inputs, n_filters=n_filters)\n",
        "  p = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(f)\n",
        "  p = tf.keras.layers.Dropout(0.3)(p)\n",
        "\n",
        "  return f, p\n",
        "\n",
        "\n",
        "def encoder(inputs):\n",
        "  '''\n",
        "  This function defines the encoder or downsampling path.\n",
        "\n",
        "  Args:\n",
        "    inputs (tensor) -- batch of input images\n",
        "\n",
        "  Returns:\n",
        "    p4 - the output maxpooled features of the last encoder block\n",
        "    (f1, f2, f3, f4) - the output features of all the encoder blocks\n",
        "  '''\n",
        "  f1, p1 = encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3)\n",
        "  f2, p2 = encoder_block(p1, n_filters=128, pool_size=(2,2), dropout=0.3)\n",
        "  f3, p3 = encoder_block(p2, n_filters=256, pool_size=(2,2), dropout=0.3)\n",
        "  f4, p4 = encoder_block(p3, n_filters=512, pool_size=(2,2), dropout=0.3)\n",
        "\n",
        "  return p4, (f1, f2, f3, f4)"
      ],
      "metadata": {
        "id": "vnqUbicBl3QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bottleneck\n",
        "A bottleneck follows the encoder block and is used to extract more features. This does not have a pooling layer so the dimensionality remains the same. You can use the conv2d_block() function defined earlier to implement this."
      ],
      "metadata": {
        "id": "XvxWqZN4mAXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bottleneck(inputs):\n",
        "  '''\n",
        "  This function defines the bottleneck convolutions to extract more features before the upsampling layers.\n",
        "  '''\n",
        "  bottle_neck = conv2d_block(inputs, n_filters=1024)\n",
        "\n",
        "  return bottle_neck"
      ],
      "metadata": {
        "id": "98NQ7q4zmDHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder\n",
        "Finally, we have the decoder which upsamples the features back to the original image size. At each upsampling level, you will take the output of the corresponding encoder block and concatenate it before feeding to the next decoder block.\n",
        "\n",
        "### Creating Decoder  utilities with the following two functions:"
      ],
      "metadata": {
        "id": "o_WtIu20mGr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder Utilities\n",
        "\n",
        "def decoder_block(inputs, conv_output, n_filters=64, kernel_size=3, strides=3, dropout=0.3):\n",
        "  '''\n",
        "  defines the one decoder block of the UNet\n",
        "\n",
        "  Args:\n",
        "    inputs (tensor) -- batch of input features\n",
        "    conv_output (tensor) -- features from an encoder block\n",
        "    n_filters (int) -- number of filters\n",
        "    kernel_size (int) -- kernel size\n",
        "    strides (int) -- strides for the deconvolution/upsampling\n",
        "    padding (string) -- \"same\" or \"valid\", tells if shape will be preserved by zero padding\n",
        "\n",
        "  Returns:\n",
        "    c (tensor) -- output features of the decoder block\n",
        "  '''\n",
        "  u = tf.keras.layers.Conv2DTranspose(n_filters, kernel_size, strides = strides, padding = 'same')(inputs)\n",
        "  c = tf.keras.layers.concatenate([u, conv_output])\n",
        "  c = tf.keras.layers.Dropout(dropout)(c)\n",
        "  c = conv2d_block(c, n_filters, kernel_size=3)\n",
        "\n",
        "  return c\n",
        "\n",
        "\n",
        "def decoder(inputs, convs, output_channels):\n",
        "  '''\n",
        "  Defines the decoder of the UNet chaining together 4 decoder blocks.\n",
        "\n",
        "  Args:\n",
        "    inputs (tensor) -- batch of input features\n",
        "    convs (tuple) -- features from the encoder blocks\n",
        "    output_channels (int) -- number of classes in the label map\n",
        "\n",
        "  Returns:\n",
        "    outputs (tensor) -- the pixel wise label map of the image\n",
        "  '''\n",
        "\n",
        "  f1, f2, f3, f4 = convs\n",
        "\n",
        "  c6 = decoder_block(inputs, f4, n_filters=512, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "  c7 = decoder_block(c6, f3, n_filters=256, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "  c8 = decoder_block(c7, f2, n_filters=128, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "  c9 = decoder_block(c8, f1, n_filters=64, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "\n",
        "  outputs = tf.keras.layers.Conv2D(output_channels, (1, 1), activation='softmax')(c9)\n",
        "\n",
        "  return outputs\n"
      ],
      "metadata": {
        "id": "JLt3waN2mJgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting it all together\n",
        "We can finally build the UNet by chaining the encoder, bottleneck, and decoder. We will specify the number of output channels and in this particular set, that would be 3. That is because there are three possible labels for each pixel: 'pet', 'background', and 'outline'."
      ],
      "metadata": {
        "id": "gjMycUhmmQLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "def unet():\n",
        "  '''\n",
        "  Defines the UNet by connecting the encoder, bottleneck and decoder.\n",
        "  '''\n",
        "\n",
        "  # specify the input shape\n",
        "  inputs = tf.keras.layers.Input(shape=(128, 128,3,))\n",
        "\n",
        "  # feed the inputs to the encoder\n",
        "  encoder_output, convs = encoder(inputs)\n",
        "\n",
        "  # feed the encoder output to the bottleneck\n",
        "  bottle_neck = bottleneck(encoder_output)\n",
        "\n",
        "  # feed the bottleneck and encoder block outputs to the decoder\n",
        "  # specify the number of classes via the `output_channels` argument\n",
        "  outputs = decoder(bottle_neck, convs, output_channels=OUTPUT_CHANNELS)\n",
        "\n",
        "  # create the model\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  return model\n",
        "\n",
        "# instantiate the model\n",
        "model = unet()\n",
        "\n",
        "# see the resulting model architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "TtYVCdd6mTX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.utils.plot_model(model, show_shapes=False)"
      ],
      "metadata": {
        "id": "fQbXjGz0sqg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile and Train the model\n",
        "Now, all that is left to do is to compile and train the model. The loss we will use is sparse_categorical_crossentropy. The reason is that the network is trying to assign each pixel a label, just like a multi-class prediction. In the true segmentation mask, each pixel has either a {0,1,2}. The network here is outputting three channels. Essentially, each channel is trying to learn to predict a class and sparse_categorical_crossentropy is the recommended loss for such a scenario."
      ],
      "metadata": {
        "id": "lQlyHJIQmanu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configure the optimizer, loss and metrics for training\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "7Lz23OHFmaWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure the training parameters and train the model\n",
        "\n",
        "TRAIN_LENGTH = info.splits['train'].num_examples\n",
        "EPOCHS = 10\n",
        "VAL_SUBSPLITS = 5\n",
        "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
        "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n",
        "\n",
        "# this will take around 20 minutes to run\n",
        "model_history = model.fit(train_dataset, epochs=EPOCHS,\n",
        "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                          validation_steps=VALIDATION_STEPS,\n",
        "                          validation_data=test_dataset)"
      ],
      "metadata": {
        "id": "f0y9FARXmhGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot the training and validation loss to see how the training went. This should show generally decreasing values per epoch."
      ],
      "metadata": {
        "id": "Ca1Yx9hYmlFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Learning curve from model history"
      ],
      "metadata": {
        "id": "vDZwgAYZf366"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_learning_curves(model_history):\n",
        "  acc = model_history.history[\"accuracy\"]\n",
        "  val_acc = model_history.history[\"val_accuracy\"]\n",
        "  loss = model_history.history[\"loss\"]\n",
        "  val_loss = model_history.history[\"val_loss\"]\n",
        "  epochs_range = range(EPOCHS)\n",
        "\n",
        "  fig = plt.figure(figsize=(8,5))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(epochs_range, acc, label=\"train accuracy\")\n",
        "  plt.plot(epochs_range, val_acc, label=\"validataion accuracy\")\n",
        "  plt.title(\"Accuracy\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(epochs_range, loss, label=\"train loss\")\n",
        "  plt.plot(epochs_range, val_loss, label=\"validataion loss\")\n",
        "  plt.title(\"Loss\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc=\"upper right\")\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "dcLiVZDpgAHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display learning curves\n",
        "display_learning_curves(model_history)"
      ],
      "metadata": {
        "id": "LlpMd04jgBcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make predictions\n",
        "The model is now ready to make some predictions. We will use the test dataset prepared earlier to feed input images that the model has not seen before. The utilities below will help in processing the test dataset and model predictions."
      ],
      "metadata": {
        "id": "TTpGSAcdmrpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Few calculations that are being used inside utilities functions\n",
        "print(info.splits['test'].num_examples)\n",
        "print(BATCH_SIZE)\n",
        "print(info.splits['test'].num_examples % BATCH_SIZE)\n",
        "print((info.splits['test'].num_examples - (info.splits['test'].num_examples % BATCH_SIZE)))\n",
        "3648/64"
      ],
      "metadata": {
        "id": "kz5MuvXx3GdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction Utilities"
      ],
      "metadata": {
        "id": "aZHDomM6wkT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_image_and_annotation_arrays():\n",
        "  '''\n",
        "  Unpacks the test dataset and returns the input images and segmentation masks\n",
        "  '''\n",
        "  images = []\n",
        "  y_true_segments = []\n",
        "  for image, annotation in test:\n",
        "    y_true_segments.append(annotation.numpy())\n",
        "    images.append(image.numpy())\n",
        "\n",
        "  y_true_segments = y_true_segments[:(info.splits['test'].num_examples - (info.splits['test'].num_examples % BATCH_SIZE))]\n",
        "\n",
        "  return images[:(info.splits['test'].num_examples - (info.splits['test'].num_examples % BATCH_SIZE))], y_true_segments"
      ],
      "metadata": {
        "id": "qKtrNWX0whX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(pred_mask):\n",
        "  '''\n",
        "  Creates the segmentation mask by getting the channel with the highest probability. Remember that we\n",
        "  have 3 channels in the output of the UNet. For each pixel, the predicition will be the channel with the\n",
        "  highest probability.\n",
        "  '''\n",
        "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
        "  pred_mask = pred_mask[..., tf.newaxis]\n",
        "  return pred_mask[0].numpy()"
      ],
      "metadata": {
        "id": "M2IJ8_jp9LhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(image, mask, num=1, model = model):\n",
        "  '''\n",
        "  Feeds an image to a model and returns the predicted mask.\n",
        "  '''\n",
        "\n",
        "  image = np.reshape(image,(1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "  pred_mask = model.predict(image)\n",
        "  pred_mask = create_mask(pred_mask)\n",
        "\n",
        "  return pred_mask"
      ],
      "metadata": {
        "id": "0U6OXAyemq4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the ground truth and predictions.\n",
        "\n",
        "# get the ground truth from the test set\n",
        "y_true_images, y_true_segments = get_test_image_and_annotation_arrays()\n",
        "\n",
        "# feed the test set to the model to get the predicted masks\n",
        "results = model.predict(test_dataset, steps=info.splits['test'].num_examples//BATCH_SIZE)\n",
        "results = np.argmax(results, axis=3)\n",
        "results = results[..., tf.newaxis]"
      ],
      "metadata": {
        "id": "SFKW7MXRnCqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute class-wise metrics:  IOU and Dice Score\n",
        "* **Intersection over union (IoU)**: It is known to be a good metric for measuring overlap between two bounding boxes or masks[Ground truth mask vs predicted mask]. If the prediction is completely correct, IoU = 1. The lower the IoU, the worse the prediction result.\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/IoU.jpg\" width=400px height=200px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "* **Dice score/coefficient**: It can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth.\n",
        "The Dice coefficient is 2 times The area of Overlap divided by the total number of pixels in both images.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/Dice_score.jpg\" width=450px height=200px/>\n",
        "</center>\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "STnFVPOUm4CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute class wise IoU and Dice score for a single test image and its prediction\n",
        "\n",
        "def class_wise_metrics(y_true, y_pred):\n",
        "    class_wise_iou = []\n",
        "    class_wise_dice_score = []\n",
        "\n",
        "    smoothening_factor = 0.00001\n",
        "    for i in range(3):\n",
        "        intersection = np.sum((y_pred == i) * (y_true == i))\n",
        "        y_true_area = np.sum((y_true == i))\n",
        "        y_pred_area = np.sum((y_pred == i))\n",
        "        combined_area = y_true_area + y_pred_area\n",
        "\n",
        "        iou = (intersection + smoothening_factor) / (combined_area - intersection + smoothening_factor)\n",
        "        class_wise_iou.append(iou)\n",
        "\n",
        "        dice_score =  2 * ((intersection + smoothening_factor) / (combined_area + 2 * smoothening_factor))\n",
        "        class_wise_dice_score.append(dice_score)\n",
        "\n",
        "    return class_wise_iou, class_wise_dice_score"
      ],
      "metadata": {
        "id": "RLHv7FPPTSXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the utility function defined above, calculating the metrics:"
      ],
      "metadata": {
        "id": "l1045AO1m_1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the class wise metrics for all test images\n",
        "cls_wise_iou_scores = []\n",
        "cls_wise_dice_scores = []\n",
        "for i in range(len(y_true_segments)):\n",
        "    iou, dice = class_wise_metrics(y_true_segments[i], results[i])\n",
        "    cls_wise_iou_scores.append(iou)\n",
        "    cls_wise_dice_scores.append(dice)\n",
        "\n",
        "# Take average to get the final result over the test set\n",
        "cls_wise_iou = np.array(cls_wise_iou_scores).mean(axis=0).round(2)\n",
        "cls_wise_dice_score = np.array(cls_wise_dice_scores).mean(axis=0).round(2)"
      ],
      "metadata": {
        "id": "5qrKsX9P9Ew4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the IOU for each class\n",
        "class_names = [\"pet\", \"background\", \"outline\"]\n",
        "\n",
        "for idx, iou in enumerate(cls_wise_iou):\n",
        "  spaces = ' ' * (10-len(class_names[idx]) + 2)\n",
        "  print(\"{}{}{} \".format(class_names[idx], spaces, iou))"
      ],
      "metadata": {
        "id": "zJgru7QvTtsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the Dice Score for each class\n",
        "for idx, dice_score in enumerate(cls_wise_dice_score):\n",
        "  spaces = ' ' * (10-len(class_names[idx]) + 2)\n",
        "  print(\"{}{}{} \".format(class_names[idx], spaces, dice_score))"
      ],
      "metadata": {
        "id": "6hIgld2xnJcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show Predictions"
      ],
      "metadata": {
        "id": "UMbvbLnznMaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please input a number between 0 to 3647 to pick an image from the dataset\n",
        "integer_slider = 360\n",
        "\n",
        "# Get the prediction mask\n",
        "y_pred_mask = make_predictions(y_true_images[integer_slider], y_true_segments[integer_slider], model = model)"
      ],
      "metadata": {
        "id": "f0aUMX2X_CoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "# Original Image\n",
        "img_arr = tf.keras.preprocessing.image.array_to_img(y_true_images[integer_slider])\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(img_arr)\n",
        "\n",
        "# Ground truth Mask\n",
        "img_arr = tf.keras.preprocessing.image.array_to_img(y_true_segments[integer_slider])\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"True Mask\")\n",
        "plt.imshow(img_arr)\n",
        " # Predicted Mask\n",
        "img_arr = tf.keras.preprocessing.image.array_to_img(y_pred_mask )\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Predicted Mask\")\n",
        "plt.imshow(img_arr)"
      ],
      "metadata": {
        "id": "Ca8TW_4zL504"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the class wise metrics\n",
        "iou, dice_score = class_wise_metrics(y_true_segments[integer_slider], y_pred_mask)\n",
        "iou, dice_score"
      ],
      "metadata": {
        "id": "HdXHeYAxHkNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implement DeepLabV3+**"
      ],
      "metadata": {
        "id": "pl_a9FJ0IcJL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEPE701wIRKX"
      },
      "source": [
        "Downsampling is widely adopted in deep convolutional neural networks (CNN) for reducing memory consumption while preserving the transformation invariance to some degree.\n",
        "\n",
        "Multiple downsampling of a CNN will lead the feature map resolution to become smaller, resulting in lower prediction accuracy and loss of boundary information in semantic segmentation.\n",
        "\n",
        "DeepLabv3+ helps in solving these issues by including **atrous convolutions**. They aggregates context around a feature which helps in segmenting it better.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **Atrous Convolution/Dilated Convolution**\n",
        "\n",
        "It is a tool for refining the effective field of view of the convolution. It modifies the field of view using a parameter termed ***atrous rate*** or ***dilation rate (d)***.\n",
        "\n",
        "With dilated convolution, as we go deeper in the network, we can keep the stride constant but with larger field-of-view without increasing the number of parameters or the amount of computation. It also enables larger output feature maps, which is useful for semantic segmentation.\n",
        "\n",
        "In the below figure, Atrous/Dilated Convolution has wider field of view with same number of parameters as Normal convolution. Only the pink ones will be consider, green ones will be ignored.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/Dilated_Conv.jpg\" width=500px>\n",
        "<br><br>\n",
        "\n",
        "\n",
        "\n",
        "#### **DeepLabv3+**\n",
        "\n",
        "Earlier version, DeepLabv3 has a problem of consuming too much time to process high-resolution images. DeepLabv3+ is a semantic segmentation architecture that improves upon DeepLabv3 with several improvements, such as adding an effective decoder module to refine the segmentation results.\n",
        "\n",
        "The below figure shows the typical architecture of DeepLabv3+. The encoder module processes multiscale contextual information by applying dilated/atrous convolution at multiple scales, while the decoder module refines the segmentation results along object boundaries.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/deeplabv3_plus_diagram.png\" >\n",
        "<br><br>\n",
        "\n",
        "Deeplabv3+ employs Aligned Xception network as its main feature extractor (encoder), although with substantial modifications. Depth-wise separable convolution replaces all max pooling procedures.\n",
        "\n",
        "The reason for using **Dilated Spatial Pyramid Pooling** is that it was shown that as the sampling rate becomes larger, the number of valid filter weights (i.e., weights that are applied to the valid feature region, instead of padded zeros) becomes smaller.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Model Playground, we can select feature extraction (encoding) network to use as either **Resnet** or EfficientNet.\n",
        "\n",
        "For our model, we use the below architecture.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/deeplabv3_plus_model.png\" width=1000px>\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "X839R080hMaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we use ResNet-50 as the backbone network, let's check the different layers present in it."
      ],
      "metadata": {
        "id": "1A9_lkz8lJod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ResNet-50 architecture for explore purpose\n",
        "res_input = keras.Input(shape=(128, 128, 3))\n",
        "resnet50 = keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_tensor = res_input)\n",
        "\n",
        "# Layers present in ResNet-50 network\n",
        "resnet50.summary()"
      ],
      "metadata": {
        "id": "baedkm2Ymvtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above layers,\n",
        "\n",
        "- Use the low-level features from the `conv2_block3_2_relu` layer of the ResNet-50 network to fead in Decoder.\n",
        "\n",
        "- Use the features from the `conv4_block6_2_relu` layer of the ResNet-50 to fead in Dilated Spatial Pyramid Pooling module.\n",
        "\n"
      ],
      "metadata": {
        "id": "3viRYy34l-AB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a function, `convolution_block()`, to add a convolution layer, a BatchNormalization layer, and apply ReLu activation in one go."
      ],
      "metadata": {
        "id": "51JFQYafBWKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convolution_block(block_input, num_filters=256, kernel_size=3, dilation_rate=1, padding=\"same\", use_bias=False):\n",
        "    x = layers.Conv2D(num_filters, kernel_size=kernel_size, dilation_rate=dilation_rate,\n",
        "                      padding=\"same\", use_bias=use_bias,\n",
        "                      kernel_initializer=keras.initializers.HeNormal())(block_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    return tf.nn.relu(x)"
      ],
      "metadata": {
        "id": "swj5dBq3Nuhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create another function to perform Dilated Spatial Pyramid Pooling. Use above function to add different convolution blocks."
      ],
      "metadata": {
        "id": "oZ97G7zKDEwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ak0-MaLIRKX"
      },
      "outputs": [],
      "source": [
        "def DilatedSpatialPyramidPooling(dspp_input):\n",
        "    dims = dspp_input.shape\n",
        "\n",
        "    # 1x1 Conv rate=1\n",
        "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
        "    # 3x3 Conv rate=6\n",
        "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
        "    # 3x3 Conv rate=12\n",
        "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
        "    # 3x3 Conv rate=18\n",
        "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
        "\n",
        "    # Image pooling\n",
        "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
        "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
        "    out_pool = layers.UpSampling2D(size = (dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation = \"bilinear\")(x)\n",
        "\n",
        "    # Concat\n",
        "    resultant = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
        "\n",
        "    return resultant\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Encoder\n",
        "\n",
        "Create a function to implement the architecture for Encoder block. Use **ResNet50** pretrained on ImageNet as the backbone network. Use the features from the `conv4_block6_2_relu` layer of the backbone to fead in Dilated Spatial Pyramid Pooling module. Then return the backbone network along with encoder output."
      ],
      "metadata": {
        "id": "W877DUELcmAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Encoder(model_input):\n",
        "    # Backbone network\n",
        "    resnet50 = keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_tensor=model_input)\n",
        "    # Features from backbone network to fead in DSPP\n",
        "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
        "    # DSPP module\n",
        "    concat_out = DilatedSpatialPyramidPooling(x)\n",
        "    # 1x1 Conv\n",
        "    output = convolution_block(concat_out, kernel_size=1)\n",
        "\n",
        "    return resnet50, output\n"
      ],
      "metadata": {
        "id": "hF3LCKGtPIlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISzIfZnCIRKY"
      },
      "source": [
        "### Create Decoder\n",
        "\n",
        "Create a function to implement the architecture for Decoder block. The encoder features are first bilinearly upsampled by a factor 4, and then concatenated with the corresponding low-level features (the `conv2_block3_2_relu` layer) from the network backbone that have the same spatial resolution.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Decoder(image_size, back_network, x):\n",
        "    # Output from Encoder, upsample by 4\n",
        "    input_a = layers.UpSampling2D(size = (image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
        "                                  interpolation = \"bilinear\")(x)\n",
        "    # Low-level features from backbone network\n",
        "    input_b = back_network.get_layer(\"conv2_block3_2_relu\").output\n",
        "    # Add 1x1 Conv on low-level features\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "    # Concat\n",
        "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
        "    # Add 3x3 Conv blocks\n",
        "    x = convolution_block(x)\n",
        "    x = convolution_block(x)\n",
        "\n",
        "    # Resultant upsample by 4\n",
        "    output = layers.UpSampling2D(size = (image_size // x.shape[1], image_size // x.shape[2]),\n",
        "                            interpolation = \"bilinear\")(x)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "6xj8SRKFPM4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Model\n",
        "\n",
        "Create a function to implement DeepLabV3+ architecture."
      ],
      "metadata": {
        "id": "ZcWB2suiRFYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DeeplabV3Plus(image_size, num_classes):\n",
        "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
        "    # Encoder part\n",
        "    back_network, x = Encoder(model_input)\n",
        "    # Decoder part\n",
        "    x = Decoder(image_size, back_network, x)\n",
        "\n",
        "    # Output/prediction layer\n",
        "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
        "\n",
        "    return keras.Model(inputs=model_input, outputs=model_output)\n"
      ],
      "metadata": {
        "id": "F55hQulMN85C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WPwmoG9IRKY"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "deeplab_model = DeeplabV3Plus(image_size = 128, num_classes = 3)\n",
        "deeplab_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-wTaL2GIRKZ"
      },
      "source": [
        "### Training\n",
        "\n",
        "We train the model using sparse categorical crossentropy as the loss function, and\n",
        "Adam as the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "deeplab_model.compile(optimizer = keras.optimizers.Adam(learning_rate=0.001),\n",
        "                      loss = loss,\n",
        "                      metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "lHOIcWvJJipS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure the training parameters and train the model\n",
        "\n",
        "TRAIN_LENGTH = info.splits['train'].num_examples\n",
        "EPOCHS = 20\n",
        "VAL_SUBSPLITS = 5\n",
        "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
        "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n",
        "\n",
        "deeplab_model_history = deeplab_model.fit(train_dataset, epochs=EPOCHS,\n",
        "                                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                                          validation_steps=VALIDATION_STEPS,\n",
        "                                          validation_data=test_dataset)"
      ],
      "metadata": {
        "id": "7ctUtBkHehbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display learning curves\n",
        "display_learning_curves(deeplab_model_history)"
      ],
      "metadata": {
        "id": "qOC3rLp3Ii6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the predictions\n",
        "\n",
        "# feed the test set to the deeplab model to get the predicted masks\n",
        "deeplab_results = deeplab_model.predict(test_dataset, steps=info.splits['test'].num_examples//BATCH_SIZE)\n",
        "deeplab_results = np.argmax(deeplab_results, axis=3)\n",
        "deeplab_results = deeplab_results[..., tf.newaxis]"
      ],
      "metadata": {
        "id": "Eez0Ct-JfBKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate the metrics"
      ],
      "metadata": {
        "id": "RwHvvl4XUNhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the class wise metrics for all test images\n",
        "deeplab_cls_wise_iou_scores = []\n",
        "deeplab_cls_wise_dice_scores = []\n",
        "for i in range(len(y_true_segments)):\n",
        "    iou, dice = class_wise_metrics(y_true_segments[i], deeplab_results[i])\n",
        "    deeplab_cls_wise_iou_scores.append(iou)\n",
        "    deeplab_cls_wise_dice_scores.append(dice)\n",
        "\n",
        "# Take average to get the final result over the test set\n",
        "deeplab_cls_wise_iou = np.array(deeplab_cls_wise_iou_scores).mean(axis=0).round(2)\n",
        "deeplab_cls_wise_dice_score = np.array(deeplab_cls_wise_dice_scores).mean(axis=0).round(2)"
      ],
      "metadata": {
        "id": "CkTwcOAGUNhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the IOU for each class\n",
        "class_names = [\"pet\", \"background\", \"outline\"]\n",
        "\n",
        "for idx, iou in enumerate(deeplab_cls_wise_iou):\n",
        "  spaces = ' ' * (10-len(class_names[idx]) + 2)\n",
        "  print(\"{}{}{} \".format(class_names[idx], spaces, iou))"
      ],
      "metadata": {
        "id": "j_1sW0nvVOjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the Dice Score for each class\n",
        "for idx, dice_score in enumerate(deeplab_cls_wise_dice_score):\n",
        "  spaces = ' ' * (10-len(class_names[idx]) + 2)\n",
        "  print(\"{}{}{} \".format(class_names[idx], spaces, dice_score))"
      ],
      "metadata": {
        "id": "n1mOjcnAVWDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare with UNet"
      ],
      "metadata": {
        "id": "CvSeGtyuWLi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot bar chart to show IoU scores for predictions from both models\n",
        "\n",
        "fig = plt.figure(figsize =(6,4))\n",
        "X = np.arange(3)\n",
        "plt.bar(X + 0.25, deeplab_cls_wise_iou, color = 'g', width = 0.25, label = 'Deeplabv3+')\n",
        "plt.bar(class_names, cls_wise_iou, color = 'b', width = 0.25, label = 'UNet')\n",
        "plt.xlabel('Class label', fontsize = 12)\n",
        "plt.ylabel('IoU score', fontsize = 12)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vDYt3G-VW-ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot bar chart to show Dice scores for predictions from both models\n",
        "\n",
        "fig = plt.figure(figsize =(6,4))\n",
        "X = np.arange(3)\n",
        "plt.bar(X + 0.25, deeplab_cls_wise_dice_score, color = 'g', width = 0.25, label = 'Deeplabv3+')\n",
        "plt.bar(class_names, cls_wise_dice_score, color = 'b', width = 0.25, label = 'UNet')\n",
        "plt.xlabel('Class label', fontsize = 12)\n",
        "plt.ylabel('Dice score', fontsize = 12)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CZxc-GgnaQil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show Predictions"
      ],
      "metadata": {
        "id": "Ii_uUg_8UNho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please input a number between 0 to 3647 to pick an image from the dataset\n",
        "integer_slider = 360\n",
        "\n",
        "# Get the prediction mask\n",
        "y_pred_mask_deeplab = make_predictions(y_true_images[integer_slider], y_true_segments[integer_slider], model = deeplab_model)"
      ],
      "metadata": {
        "id": "K4eV7hEJUNhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "# Original Image\n",
        "img_arr = tf.keras.preprocessing.image.array_to_img(y_true_images[integer_slider])\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(img_arr)\n",
        "\n",
        "# Ground truth Mask\n",
        "img_arr = tf.keras.preprocessing.image.array_to_img(y_true_segments[integer_slider])\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"True Mask\")\n",
        "plt.imshow(img_arr)\n",
        " # Predicted Mask\n",
        "img_arr = tf.keras.preprocessing.image.array_to_img(y_pred_mask_deeplab)\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Predicted Mask\")\n",
        "plt.imshow(img_arr)"
      ],
      "metadata": {
        "id": "IX3U8hV5Vm0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the class wise metrics\n",
        "dlab_iou, dlab_dice_score = class_wise_metrics(y_true_segments[integer_slider], y_pred_mask_deeplab)\n",
        "dlab_iou, dlab_dice_score"
      ],
      "metadata": {
        "id": "JEhW6DbSVsWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "outputs": [],
      "source": [
        "#@title The encoder also returns the complete resnet50 as an output while implementing Deeplabv3+ in this notebook. In the decoder, what layer is used from that resnet 50? {run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"conv4_block6_2_relu\" #@param [\"\", \"conv4_block6_2_relu\", \"conv2_block3_2_relu\", \"the last layer of resnet50 i.e. conv5_block3_out\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"No\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ]
}