{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuvayan/AIMLOPS/blob/main/M5_AST_03_TextVectorization_and_Embedding_Layers_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 3: TextVectorization & Embedding Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand the big picture of transformers\n",
        "* understand and work with the TextVectorization layer\n",
        "* understand and work with the Embedding layer\n",
        "* learn word embeddings during model training\n",
        "* perform visualization of word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Big Picture of Transformer"
      ],
      "metadata": {
        "id": "Tv8TQrtfwaNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextVectorization and Embedding Layers are used in Encoder-Decoder Transformer.\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST%205%20Big%20Picture.png\" width=800px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "ni6YCBwE7d_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is the entire architecture of transformer. A TextVectorization layer, Embedding layer, an Encoder and a Decoder.\n",
        "\n",
        "Transformer architecture follows an encoder-decoder structure. The encoder, on the left-hand side, is tasked with mapping an input sequence to a sequence of continuous representations; the decoder, on the right-hand side, receives the output of the encoder together with the decoder output at the previous time step to generate an output sequence."
      ],
      "metadata": {
        "id": "AxzUg64Jdv-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer architecture was originally designed for translation. In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence). The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word.\n",
        "\n",
        "To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!). For instance, when trying to predict the fourth word, the attention layer will only have access to the words in positions 1 to 3."
      ],
      "metadata": {
        "id": "rgVPmcTjnORs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextVectorization and Embedding Layers are also required in Encoder-only Transformer.\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5_AST_05_Image1_Transformer.png\" width=900px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "VtUillZR7eEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment encoder & decoder will not form the topic of discussion, the main focus will be on the TextVectorization and Embedding Layers.\n",
        "This has been discussed in detail in the later sections of this notebook."
      ],
      "metadata": {
        "id": "geRywC9LeHjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Description"
      ],
      "metadata": {
        "id": "ECysu18IoMaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **IMDb Movie Reviews dataset** is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as *positive* or *negative*. The dataset contains an even number of positive and negative reviews."
      ],
      "metadata": {
        "id": "XLrl_aUaoPam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is processed and used in the later sections of this notebook."
      ],
      "metadata": {
        "id": "KzZ2UaHyoS6z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4UlAXa6hWqZ"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mAxYYRyhWq3"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2P_-RHtFhWq3"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M5_AST_03_TextVectorization_and_Embedding_Layers_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    ipython.magic(\"sx curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n",
        "    ipython.magic(\"sx tar -xvzf aclImdb_v1.tar.gz\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RH8Ecq9sbYU"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, pathlib, shutil, random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, Dense\n",
        "from tensorflow.keras.utils import text_dataset_from_directory\n",
        "\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "qnUhfGmx9cup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TextVectorization"
      ],
      "metadata": {
        "id": "KtDuXYqa7cbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It invloves preparing the text data:\n",
        "  * Text standardization\n",
        "  * Text splitting into tokens\n",
        "  * Vocabulary indexing\n",
        "  \n"
      ],
      "metadata": {
        "id": "TIv-Yv3EnQ4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A flowchart depicting the procedure or sequence of steps followed in a TextVectorization layer.\n",
        "* 'Standardization' is taking care of basic preprocessing of text data such as removing the punctuation and converting the text to lower case.\n",
        "* 'Tokenization' is giving the list of words from the sentence.\n",
        "* Later, these words are represented with indices and with the help of embedding to get the vector encoding of indices."
      ],
      "metadata": {
        "id": "o590tI2BldJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5_AST_05_Transformer_Encoder_Text_data_prep.png\" width=650px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "y-m7SHDzoHHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All these steps are performed in a TextVectorization Layer.\n",
        "\n",
        "\n",
        "*   Keras provides a TextVectorization layer which can be dropped directly into\n",
        "      - a tf.data pipeline **or**\n",
        "      - a Keras model\n",
        "\n",
        "*  MOREOVER, TextVectorization also handles both approaches of representing groups of words:\n",
        "      - Words as a set or Bag-of-words\n",
        "      - Words as a sequence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_RGm-7ySooG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a dummy dataset and a test sentence\n"
      ],
      "metadata": {
        "id": "8QdHTVk5K_wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "# dataset_t = [\"I write, rewrite, and still rewrite again\"]\n",
        "#Q: Is the word 'still' in the dataset (vocabulary)? Is it there in the test_sentence?\n",
        "#Q: How many words in test_sentence?"
      ],
      "metadata": {
        "id": "njuS3JqcS-RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a TextVectorization layer and adapt to dummy dataset"
      ],
      "metadata": {
        "id": "M9sp6faVpYYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and demonstrate the working of a TextVectorization layer."
      ],
      "metadata": {
        "id": "pB0aHkyhr3D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q: What 3 things does a TextVec layer do?\n",
        "\n",
        "# Instantiating a TextVectorization layer/object with output mode as integer\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",              # int is default. There are different kinds of modes available\n",
        "    max_tokens=15,                  # Vocabulary size\n",
        "    output_sequence_length=10,      # Maximum length of output sequence\n",
        "    # We can use custom functions also for standardizing and splitting the text - see the Book by Chollet\n",
        "    # standardize=custom_standardization_fn,\n",
        "    # split=custom_split_fn,\n",
        ")\n",
        "\n",
        "# Adapt to data\n",
        "text_vectorization.adapt(dataset)      # Computes a vocabulary of string terms from tokens in a dataset\n"
      ],
      "metadata": {
        "id": "q0AS4G6RqMEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To see the working of TextVectorization\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "print(f\"vocabulary = {vocabulary}\")\n",
        "print(f\"len(vocabulary) = {len(vocabulary)}\")\n",
        "\n",
        "# To see how the the text_vec layer transforms/vectorizes the raw text\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(f\"encoded sentence = {encoded_sentence}\")\n",
        "print(f\"len(encoded sentence) = {len(encoded_sentence)}\")\n",
        "# print(f\"encoded dataset_t = {text_vectorization(dataset_t)}\")\n",
        "\n",
        "# decode back for comparison with test_sentence\n",
        "inverse_vocab = dict(enumerate(vocabulary)) # making a dictionary to decode embeddings\n",
        "print(f\"inverse_vocab = {inverse_vocab}\")\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(f\"decoded sentence = {decoded_sentence}\")\n",
        "\n",
        "print(f\"test_sentence = {test_sentence}\")\n",
        "\n",
        "# Q: What is a vocabulary?\n",
        "# Q: No. of tokens in vocabulary?\n",
        "# Q: Length of encoded_sentence (output of TextVec layer)?\n",
        "# Q: Type of elements in encoded_sentence (embedding)?\n",
        "# Q: Is decoded sentence the same as the test_sentence? Why?"
      ],
      "metadata": {
        "id": "eqL6sCaWTnTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing the dataset using TextVectorization layer of keras"
      ],
      "metadata": {
        "id": "flOZCU1JQNYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "UkoDTKmd900K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pre-processed version of the IMDB dataset provided by Keras was used in the previous assignments.\n",
        "\n",
        "Originally IMDB dataset contains the *train* and the *test* folders.\n",
        "Here, the original dataset will be used and pre-processing related to it will be explored."
      ],
      "metadata": {
        "id": "3XIAoz7enlj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List subdirectories\n",
        "!cd aclImdb && ls -d */"
      ],
      "metadata": {
        "id": "46rYfHyZrqbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove unnecessary folder\n",
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "mFgcefQNLh-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise a sample\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "id": "-DWQXmaYLuha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a validation directory and move 20% of the train data to it"
      ],
      "metadata": {
        "id": "Za7BufC_2q_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# move 20% of the training data to the validation folder\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    # random.Random(1337).shuffle(files) # We should shuffle. Only commenting for demonstration\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ],
      "metadata": {
        "id": "9BseO3_sLukB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create batches of data using `text_dataset_from_directory`"
      ],
      "metadata": {
        "id": "YBeeGi5S8pbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset using utility\n",
        "batch_size = 32\n",
        "\n",
        "# Q: Name other such utilities seen earlier ?\n",
        "train_ds = text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size)\n",
        "\n",
        "val_ds = text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
        "\n",
        "test_ds = text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)\n",
        "\n",
        "# Extracting only the review text(not labels); to be used later to adapt the TextVec layer\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)             # lambda x, y: x  --> replace x,y with x. That is remove labels, just keep text data.\n"
      ],
      "metadata": {
        "id": "cDLoEh2CLwB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 20000, 5000, and 25000 records in train, validation, and test directories with two class as positive and negative."
      ],
      "metadata": {
        "id": "htOQeKY98_3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shapes\n",
        "\n",
        "for inputs, targets in train_ds:\n",
        "\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "\n",
        "    print(\"inputs[2]:\", inputs[2])\n",
        "    print(\"targets[2]:\", targets[2])\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "u_03-Oj9LwD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create TextVectorization layer and adapt to dataset"
      ],
      "metadata": {
        "id": "sR63E5Ea9fDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing the data\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,    # Q: What is the vocabular size?\n",
        "    output_mode=\"int\",        # Q: What will be the type of output for a token (say), 'amazing' ?\n",
        "    output_sequence_length=max_length,      # Q: What is the maximum length of review? Is it a fair assumption?\n",
        "    )\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "\n",
        "# Apply TextVec to train, val, test set\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), tf.reshape(y, (-1,1))),\n",
        "                            num_parallel_calls=4)\n",
        "\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), tf.reshape(y, (-1,1))),\n",
        "                        num_parallel_calls=4)\n",
        "\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), tf.reshape(y, (-1,1))),\n",
        "                          num_parallel_calls=4)\n"
      ],
      "metadata": {
        "id": "Bqi5Z24gMK9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize and compare the raw and processed data"
      ],
      "metadata": {
        "id": "J7sj-tcc9v2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize the raw text and the vectorized (to int) text\n",
        "for text, label in train_ds:\n",
        "  print(text[0])\n",
        "  print(label[0])\n",
        "  break\n",
        "\n",
        "for int_of_text, label in int_train_ds:\n",
        "  print(int_of_text[0])\n",
        "  print(label[0])\n",
        "  break\n",
        "\n",
        "# Q: How can you verify whether the index of movie is 18?\n"
      ],
      "metadata": {
        "id": "nMUYGUqBx3K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector representation of the word 'movie'"
      ],
      "metadata": {
        "id": "P01-UwZIAuzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization(\"movie\")\n",
        "# Q: What is the shape of the TV output?\n",
        "# Q: Why so many 0s?\n"
      ],
      "metadata": {
        "id": "Jl8iSY55z1A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector representation of \"great movie\" and \"a fine story\""
      ],
      "metadata": {
        "id": "6oAL6gzUAzvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization([\"great movie\", \"a fine story\"])\n",
        "#Q: shape?"
      ],
      "metadata": {
        "id": "oCDXUf8bDRIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embeddings"
      ],
      "metadata": {
        "id": "BL3w3DRqVy5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why do we need Word Embeddings?**\n",
        "\n",
        "To deal with textual data, we need to convert it into numbers before feeding it into any machine learning model. For simplicity, words can be compared to categorical variables. We use one-hot encoding to convert categorical features into numbers. To do so, we create dummy features for each of the category and populate them with 0's and 1's.\n",
        "\n",
        "Similarly, if we use one-hot encoding on words in textual data, we will have a dummy feature for each word, which means 10,000 features for a vocabulary of 10,000 words. This is not a feasible embedding approach as it demands large storage space for the word vectors and reduces model efficiency and no relation is captured between words.\n",
        "\n",
        "**Word embeddings** are vector representations of words that achieve exactly this: they map human language into a structured geometric space.\n",
        "\n",
        "* dense (floats)\n",
        "* low-dimensional (1024 dims for large vocabs)"
      ],
      "metadata": {
        "id": "ex3lQf00YyM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two ways to obtain word embeddings:\n",
        "\n",
        "* Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors, in the same way you learn the weights of a neural network. **Move away from manual feature engineering.**\n",
        "* Load into your model word embeddings that were precomputed using a different machine learning task than the one you’re trying to solve. These are called pretrained word embeddings.\n",
        "\n",
        "**Q: Do two ways remind you of something we studied in CNNs ?**\n",
        "\n",
        "In this assignment the main agenda is to explore the Learning of word embeddings.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fMKg5tOpcgXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Layer\n"
      ],
      "metadata": {
        "id": "t5OVct9NckRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The procedure if as follows:\n",
        "\n",
        "*   Like a dictionary that **maps integer indices** (which stand for specific words) **to dense vectors**\n",
        "\n",
        "*   Input: a rank-2 tensor of integers, of shape (batch_size, sequence_length)\n",
        "*   Output: 3D floating-point tensor of shape (batch_size, sequence_length, embedding_dimensionality)\n",
        "*   WORD INDEX ⭢ EMBEDDING LAYER ⭢ CORRESPONDING WORD VEC\n",
        "\n",
        "*   Initial weights are random\n",
        "*   Learns specialized structure upon training\n",
        "\n"
      ],
      "metadata": {
        "id": "gdLRIpXwkQAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of Word Embeddings\n",
        "\n",
        "Apply dimensionality reduction to the word embeddings to convert it into 2D. Later, plot this 2D vector.\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Word_Embedding.png\" width=\"650\" height=\"450\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "4O2LtP7V_jWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization in 3D:\n",
        "\n",
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST5%20Embedding%20Layer.png\" width=750px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "G91_UEMy-gpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a NN architecture with a TextVectorization layer, an Embedding layer, and Dense layers"
      ],
      "metadata": {
        "id": "k4mowLS1YiVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = 20000\n",
        "inputs = keras.Input(shape=(1,), dtype=tf.string)           # shape=(None,), dtype=\"int64\"\n",
        "\n",
        "# The Text Vectoritation layer\n",
        "txt_vec_out = text_vectorization(inputs)             # Note that this TextVec layer is already apadted on the train dataset\n",
        "\n",
        "# The Embedding layer\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256, name='embedding')(txt_vec_out)    # the largest integer (i.e. word index) in the input\n",
        "                                                                                                    # should be no larger than 19999 (vocabulary size).\n",
        "# Q: What is the input to the Embedding layer?\n",
        "# Q: What is the dimension of the output embeddings?\n",
        "# Q: In embedding layer shape, what are None and None?\n",
        "\n",
        "x = layers.Dense(64, activation=\"relu\")(embedded)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "#Q: Weights in the embedding layer?\n",
        "#Hint: Dict; 1 input word => embedding of size ___ ."
      ],
      "metadata": {
        "id": "7tm5OHz0Ha1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the words in 2D-plane by reducing the dimensions using PCA\n",
        "\n",
        "Use the word embeddings from before and after model training"
      ],
      "metadata": {
        "id": "bPgn8y_bDH5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the embedding layer\n",
        "embedding_layer = model.get_layer('embedding')\n",
        "\n",
        "# Get the embeddings\n",
        "embeddings = embedding_layer.get_weights()[0]\n",
        "embeddings.shape"
      ],
      "metadata": {
        "id": "Sr0WUiNtIFUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vocabulary from the TextVectorization layer\n",
        "vocab = text_vectorization.get_vocabulary()\n",
        "len(vocab)"
      ],
      "metadata": {
        "id": "WcbbZGCNRXgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample words to visualize word embeddings for\n",
        "test_words = ['good', 'bad', 'nice', 'poor', 'terrible', 'terrific', 'awesome', 'awful', 'best', 'worst']\n",
        "\n",
        "print(f\"{'Word':<15} {'Index'}\")\n",
        "print(\"=\"*30)\n",
        "for word in test_words:\n",
        "    print(f\"{word:<15} {vocab.index(word)}\")"
      ],
      "metadata": {
        "id": "D3RCOsGSRXgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding dimension\n",
        "embeddings[vocab.index('good')].shape"
      ],
      "metadata": {
        "id": "G0pgCNGOIFUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class\n",
        "# n_components in PCA specifies the no. of dimensions\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "\n",
        "# Fit and transform the vectors using PCA model\n",
        "reduced_untrained_emb = pca.fit_transform(embeddings)\n",
        "reduced_untrained_emb.shape"
      ],
      "metadata": {
        "id": "pjxK8A-o6Cdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduced embedding for word 'good'\n",
        "reduced_untrained_emb[vocab.index('good')]"
      ],
      "metadata": {
        "id": "-SJ4bmMPIFUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the embeddings\n",
        "plt.figure(figsize=(8, 6))\n",
        "for word in test_words:\n",
        "    if word != '':  # Skip the empty string token\n",
        "        x, y = reduced_untrained_emb[vocab.index(word)]\n",
        "        plt.scatter(x, y)\n",
        "        plt.annotate(word, (x, y), xytext=(5, 2), textcoords='offset points')\n",
        "\n",
        "plt.title(\"Word Embeddings Visualization (Before training)\")\n",
        "plt.xlabel(\"Dimension 1\")\n",
        "plt.ylabel(\"Dimension 2\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mOYsvNeNbjcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model *(Switch to GPU runtime if needed)*"
      ],
      "metadata": {
        "id": "1DL5Gb3tIFUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model on train set\n",
        "callbacks = [keras.callbacks.ModelCheckpoint(\"one_hot_dense.keras\", save_best_only=True)]\n",
        "\n",
        "# Change target shape from (None,) to (None, 1)\n",
        "train_dataset = train_ds.map(lambda x, y: (x, tf.reshape(y, (-1,1))))                # Note that we are using 'train_ds' and not 'int_train_ds'\n",
        "val_dataset = val_ds.map(lambda x, y: (x, tf.reshape(y, (-1,1))))\n",
        "\n",
        "model.fit(train_dataset,\n",
        "          validation_data = val_dataset,\n",
        "          epochs = 20,\n",
        "          callbacks = callbacks)"
      ],
      "metadata": {
        "id": "WGtVcVbnIFUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load saved model\n",
        "# model = keras.models.load_model(\"one_hot_dense.keras\")\n",
        "\n",
        "# Check model performance\n",
        "test_dataset = test_ds.map(lambda x, y: (x, tf.reshape(y, (-1,1))))\n",
        "print(f\"Test acc: {model.evaluate(test_dataset)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "qyC4SWhR3k1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above test accuracy, it can be seen that the model perfomance is not that well. It is expected as we are using only Dense layers.\n",
        "\n",
        "Let's see if the embeddings learned during training were able to capture the semantic relationships between words."
      ],
      "metadata": {
        "id": "ZRLocsUHLV8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the embedding layer\n",
        "trained_embedding_layer = model.get_layer('embedding')\n",
        "\n",
        "# Get the embeddings\n",
        "trained_embeddings = trained_embedding_layer.get_weights()[0]\n",
        "trained_embeddings.shape"
      ],
      "metadata": {
        "id": "D8G_TRTfIFUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class\n",
        "# n_components in PCA specifies the no.of dimensions\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "\n",
        "# Fit and transform the vectors using PCA model\n",
        "reduced_trained_emb = pca.fit_transform(trained_embeddings)\n",
        "reduced_trained_emb.shape"
      ],
      "metadata": {
        "id": "hrsYQ-5Y58nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the embeddings\n",
        "plt.figure(figsize=(8, 6))\n",
        "for word in test_words:\n",
        "    if word != '':  # Skip the empty string token\n",
        "        x, y = reduced_trained_emb[vocab.index(word)]\n",
        "        plt.scatter(x, y)\n",
        "        plt.annotate(word, (x, y), xytext=(5, 2), textcoords='offset points')\n",
        "\n",
        "plt.title(\"Word Embeddings Visualization (After training)\")\n",
        "plt.xlabel(\"Dimension 1\")\n",
        "plt.ylabel(\"Dimension 2\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-iiGLD_RIFUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot, it can be seen that good nice are more related, bad poor are more related, and so on."
      ],
      "metadata": {
        "id": "QhxPUb2WF-Nj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOC1n4C5dxsb"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HQodIl8dxs1"
      },
      "outputs": [],
      "source": [
        "#@title What is the primary purpose of word embeddings in natural language processing? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"To increase the size of text data\", \"To convert words into numerical vectors\", \"To identify grammatical errors\", \"To generate random text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ]
}