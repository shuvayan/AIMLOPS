{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuvayan/AIMLOPS/blob/main/M5_AST_05_Transformer_Decoder_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 5: Transformer Decoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand the big picture of transformers\n",
        "* explore masking of transformers\n",
        "* implement transformer decoder and understand its architecture\n",
        "* apply learning on a machine translation problem"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Big Picture of Transformer"
      ],
      "metadata": {
        "id": "Tv8TQrtfwaNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST%205%20Big%20Picture.png\" width=700px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "ni6YCBwE7d_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer architecture follows an encoder-decoder structure:\n",
        "\n",
        "- the ***encoder***, on the left-hand side, is tasked with mapping an input sequence to a sequence of continuous representations;\n",
        "- the ***decoder***, on the right-hand side, receives the output of the encoder together with the decoder output at the previous time step to generate an output sequence."
      ],
      "metadata": {
        "id": "4xCXKCLBobfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer decoder generates sequences autoregressively by attending to previously generated positions using masked self-attention, attending to the encoder's output using encoder-decoder attention, applying feed-forward networks, and utilizing positional encodings. This architecture allows the decoder to produce coherent and contextually accurate sequences in various natural language processing tasks"
      ],
      "metadata": {
        "id": "bqM7uCmtEt5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Steps:"
      ],
      "metadata": {
        "id": "rKQ0Fvl_jNqU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4UlAXa6hWqZ"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mAxYYRyhWq3"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "83515740-9e5f-44d5-b36a-5bc2439d0228",
        "id": "2P_-RHtFhWq3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=aimlops_c3&recordId=6164\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M5_AST_05_Transformer_Decoder_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    ipython.magic(\"sx wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\")\n",
        "    ipython.magic(\"sx unzip -q spa-eng.zip\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RH8Ecq9sbYU"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "Ms8SJA8jELck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part A** : Building Encoder Transformer"
      ],
      "metadata": {
        "id": "qDk0xc-_gbV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The concepts for Transformer encoder have been discussed in Assignment 4 and the same steps are implemented here for creating a decoder network."
      ],
      "metadata": {
        "id": "FuUJwJFNE8jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define TransformerEncoder class to be used in model building"
      ],
      "metadata": {
        "id": "vtZo2HU5IpgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim    # Dimension of embedding. 4 in the dummy example\n",
        "        self.dense_dim = dense_dim    # No. of neurons in dense layer\n",
        "        self.num_heads = num_heads    # No. of heads for MultiHead Attention layer\n",
        "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)    # MultiHead Attention layer\n",
        "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation=\"relu\"),\n",
        "                                            layers.Dense(embed_dim),]    # encoders are stacked on top of the other.\n",
        "                                           )                             # So output dimension is also embed_dim\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    # Call function based on figure above\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "            print(f\"**test: mask in not None. mask = {mask}\")\n",
        "\n",
        "        attention_output = self.attention(query=inputs,             # Query: inputs,\n",
        "                                          value=inputs,             # Value: inputs,\n",
        "                                          key=inputs,               # Keys: Same as Values by default\n",
        "                                          attention_mask=mask\n",
        "                                          )                         # Q: Can you see how this is self attention? A: all args are the same\n",
        "\n",
        "        proj_input = self.layernorm_1(inputs + attention_output) # LayerNormalization; + Recall cat picture\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)  # LayerNormalization + Residual connection\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "XW-WstYmH-4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Embedding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Learn position- embedding vectors the same way we learn to embed word indices.\n",
        "*   Proceed to **add** our position embeddings to the corresponding word embeddings, to obtain a position-aware word embedding.\n",
        "*   This technique is called “positional embedding.”\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WcVnGSnpSFGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST6%20Positional%20Embedding.png\" width=700px/>\n"
      ],
      "metadata": {
        "id": "0n04kGhBbQCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST6%20Encoder%20Embedding.png\" width=650px/>\n",
        "</center>\n",
        "\n",
        "![]()"
      ],
      "metadata": {
        "id": "k3VrrCylc6q4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q:** In the picture above:\n",
        "\n",
        "\n",
        "*   What is the embedding dimension for both the layers? - 3\n",
        "*   How many rows would the token embedding layer have?  - 20000 (vocab size)\n",
        "*   How many rows would the postional embedding layer have? - 600 (seq length)\n",
        "*   Where do we get the indices in token embedding layer? - from TextVectorization\n",
        "*   Where do we get the indices in token embedding layer? - We explicitly define a range\n",
        "\n"
      ],
      "metadata": {
        "id": "AMOrn4pQeefD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define PositionalEmbedding class to be used in model building"
      ],
      "metadata": {
        "id": "jSibMhQ0I3T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using positional encoding to re-inject order information\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        # input_dim = (token) vocabulary size,  output_dim = embedding size\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.token_embeddings = layers.Embedding(       # Q: what is input_dim and output_dim? A: vocab size, embedding dim\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(    # Q: Why input_dim = seq_length?  A: there are seq_len (here 600) no. of possible positions\n",
        "            input_dim=sequence_length, output_dim=output_dim)   # Q: What is the vocab for this Embedding layer ? A: seq_length\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):   # inputs will be a batch of sequences (batch, seq_len)\n",
        "        length = tf.shape(inputs)[-1]     # lenght will just be sequence length\n",
        "        positions = tf.range(start=0, limit=length, delta=1) # indices for input to positional embedding\n",
        "        embedded_tokens = tf.reshape(self.token_embeddings(inputs), (-1, length, self.output_dim))\n",
        "        embedded_positions = tf.reshape(self.position_embeddings(positions), (-1, length, self.output_dim))\n",
        "        return layers.Add()([embedded_tokens, embedded_positions])     # ADD the embeddings\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):  # makes this layer a mask-generating layer\n",
        "        if mask is None:\n",
        "            return None\n",
        "        return tf.math.not_equal(inputs, 0)     # mask will get propagated to the next layer.\n",
        "\n",
        "    # When using custom layers, this enables the layer to be reinstantiated from its config dict,\n",
        "    # which is useful during model saving and loading.\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "qCAWpi5zGUlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What does tf.math.not_equal() do?\n",
        "\n",
        "a = tf.constant([1,0,2,0,3]) # a is a tensor\n",
        "print(a)\n",
        "print(tf.math.not_equal(a, 0))   # which elements of 'a' are not equal to 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwDciZc2g3za",
        "outputId": "bf25ae94-36d3-4078-fdac-a345d9f6522d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1 0 2 0 3], shape=(5,), dtype=int32)\n",
            "tf.Tensor([ True False  True False  True], shape=(5,), dtype=bool)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TransformerEncoder model definition with Positional Embedding"
      ],
      "metadata": {
        "id": "ZVD5IzTuJD29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Combining the Transformer encoder with positional embedding\n",
        "#  The values below are for the classificaiton problem. We will change them for the tranlation example\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")  # Q: Why is the input expected to have dtype int? A: Inputs coming from TextVectorization layer.\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "print(f\"Token embedding weights: {256*15000}\")\n",
        "print(f\"Position embedding weights: {256*20}\")\n",
        "print(f\"Total no. of weights: {256*15000 + 256*20}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "7uaobuC7WZ0w",
        "outputId": "0d9df53e-8637-4665-acb9-545886f0d48d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ positional_embedding                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │       \u001b[38;5;34m3,845,120\u001b[0m │\n",
              "│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)                │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_encoder                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │         \u001b[38;5;34m543,776\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_max_pooling1d                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m257\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ positional_embedding                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,845,120</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)                │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_encoder                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">543,776</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_max_pooling1d                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,389,153\u001b[0m (16.74 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,389,153</span> (16.74 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,389,153\u001b[0m (16.74 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,389,153</span> (16.74 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embedding weights: 3840000\n",
            "Position embedding weights: 5120\n",
            "Total no. of weights: 3845120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part B** : Building Decoder Transformer"
      ],
      "metadata": {
        "id": "orBRzfHAebIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder - Decoder Overview\n",
        "\n",
        "Encoder - Encodes the input as some representation\n",
        "\n",
        "Decoder - Uses the encoded representation (and targets) to decode these representation as per the target.\n",
        "\n",
        "Encoders - can be CNNs, RNNs, FFNs\n",
        "Decoders - can be CNNs, RNNs, FFNs"
      ],
      "metadata": {
        "id": "b2Z0_zLTsYPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples:\n",
        "* Problem: Predict description in text from images.\n",
        "  - Encoder - CNN\n",
        "  - Decoder - RNN\n",
        "\n",
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST6%20Encoder%20Decoder%20Overview.png\" width=350px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "Jw9VGiLy2Nik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Problem: Language Translation\n",
        "  - Encoder - RNN\n",
        "  - Decoder - RNN\n",
        "\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST6%20Encoder%20Time%20Distributed.jpg\" width=600px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "3UxReBBp2NyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Problem: Language Translation\n",
        "    - Transformer Encoder\n",
        "    - Transformet Decoder\n",
        "\n",
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST6%20Transformer%20Network.png\" width=350px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "KzlzttwC2-I1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training,\n",
        "* An encoder model turns the source sequence into an intermediate representation.\n",
        "* **A decoder is trained to predict the next token i** in the target sequence by looking at both\n",
        "    - previous tokens (0 to i - 1) and\n",
        "    - the encoded source sequence\n",
        "    "
      ],
      "metadata": {
        "id": "d3LdyPKhsYWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During inference, we don’t have access to the target sequence—we’re trying to predict it from scratch. We’ll have to generate it one token at a time:\n",
        "1. We obtain the encoded source sequence from the encoder.\n",
        "2. The decoder starts by looking at the encoded source sequence as well as an initial “seed” token (such as the string \"[start]\"), and uses them to predict the\n",
        "first real token in the sequence.\n",
        "3. The predicted sequence so far is fed back into the decoder, which generates the next token, and so on, until it generates a stop token (such as the string \"[end]\")."
      ],
      "metadata": {
        "id": "Wq5yUiVhudHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST6%20Transformer%20gif.gif\" width=750px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "2Do0zZWCuiJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masking\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HQO73b-r4y0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Masking is needed to prevent the attention mechanism of a transformer from “cheating” in the decoder when training (on a translating task for instance). This kind of “ cheating-proof masking” is not present in the encoder side."
      ],
      "metadata": {
        "id": "7Dkb0qlNSsAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the sequence: “I love it”, then the expected prediction for the token at position one (“I”) is the token at the next position (“love”). Similarly the expected prediction for the tokens “I love” is “it”.\n",
        "\n",
        "We do not want the attention mechanism to share any information regarding the token at the next positions, when giving a prediction using all the previous tokens.\n",
        "\n",
        "To ensure that this is done, we mask future positions (setting them to -inf) before the softmax step in the self-attention calculation."
      ],
      "metadata": {
        "id": "oR2VCAGWTWHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding mask"
      ],
      "metadata": {
        "id": "dLhi72ah-9na"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding is a special form of masking where the masked steps are at the start or the end of a sequence. Padding comes from the need to encode sequence data into contiguous batches: in order to make all sequences in a batch fit a given standard length, it is necessary to pad or truncate some sequences."
      ],
      "metadata": {
        "id": "Zg2lNXdplnb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The Embedding layer is capable of generating a “mask” that corresponds to its input data.\n",
        "\n",
        "* By default, this option isn’t active—you can turn it on by passing mask_zero=True to your Embedding layer.\n",
        "\n",
        "* You can retrieve the mask with the compute_mask() method:"
      ],
      "metadata": {
        "id": "0fsvrm8dRX4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An example to understand Padding Masking"
      ],
      "metadata": {
        "id": "jvl9mkDal6zP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding mask\n",
        "embedding_layer_ = layers.Embedding(input_dim=10, output_dim=256, mask_zero=True)\n",
        "some_input = [\n",
        "  [4,3,2,1,0,0,0],\n",
        "  [5,4,3,2,1,0,0],\n",
        "  [2,1,0,0,0,0,0]]\n",
        "d_mask = embedding_layer_.compute_mask(some_input)\n",
        "print(d_mask)\n",
        "print(tf.cast(d_mask, dtype=\"int32\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Md-LHZtQ7Zp",
        "outputId": "16700952-e260-41c8-bb49-de40a5d48ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ True  True  True  True False False False]\n",
            " [ True  True  True  True  True False False]\n",
            " [ True  True False False False False False]], shape=(3, 7), dtype=bool)\n",
            "tf.Tensor(\n",
            "[[1 1 1 1 0 0 0]\n",
            " [1 1 1 1 1 0 0]\n",
            " [1 1 0 0 0 0 0]], shape=(3, 7), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Causal Padding"
      ],
      "metadata": {
        "id": "Iuyzd5P7Q5Q4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "*   The TransformerDecoder is order-agnostic: it looks at the entire target sequence at once.\n",
        "*   If it were allowed to use its entire input, it would simply learn to copy input step N+1 to location N in the output.\n",
        "*  Solution: mask the upper half of the pairwise attention matrix to prevent the model from paying any attention to information from the future\n",
        "*  We'll see this in the method get_causal_attention_mask(self, inputs) inside the decoder class\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "-TNaWfPasli3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST5%20Self%20Attention%20Scores.png\" width=600px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "1-U2aT2c58-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST5%20Multihead%20Attention.png\" width=600px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "IFAiWsE3kK7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume sequence length is 5\n",
        "j = normal_range = tf.range(5)\n",
        "i = with_new_axis = tf.range(5)[:, tf.newaxis]\n",
        "# with_2new_axis = tf.range(10)[:, tf.newaxis, tf.newaxis]"
      ],
      "metadata": {
        "id": "7DIIseZQWPsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(normal_range)\n",
        "print(with_new_axis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv-vP3TrWWaP",
        "outputId": "2eeff253-c057-4aef-e29e-66868a72a01e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [4]], shape=(5, 1), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# j is broadcasted; booleans are cast to int32\n",
        "d_mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "print(d_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nqm6dAHaF4G",
        "outputId": "41ad1538-a766-49aa-eca3-7afaef2fadd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1 0 0 0 0]\n",
            " [1 1 0 0 0]\n",
            " [1 1 1 0 0]\n",
            " [1 1 1 1 0]\n",
            " [1 1 1 1 1]], shape=(5, 5), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_mask = tf.reshape(d_mask, (1, 5, 5))\n",
        "print(d_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK_XMJCfap91",
        "outputId": "1e441b29-1d88-4438-ed34-35d870edb284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[1 0 0 0 0]\n",
            "  [1 1 0 0 0]\n",
            "  [1 1 1 0 0]\n",
            "  [1 1 1 1 0]\n",
            "  [1 1 1 1 1]]], shape=(1, 5, 5), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tile multiplier for tiling\n",
        "batch_size = 2\n",
        "mult = tf.concat(\n",
        "    [tf.expand_dims(batch_size, -1),\n",
        "      tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "print(tf.expand_dims(batch_size, -1))\n",
        "print(tf.constant([1, 1], dtype=tf.int32))\n",
        "print(mult)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSbwK1f3bIuU",
        "outputId": "43b67521-513b-40fe-950d-6ea4306516a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "tf.Tensor([1 1], shape=(2,), dtype=int32)\n",
            "tf.Tensor([2 1 1], shape=(3,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tile the mask to replicate across batchsize\n",
        "causal_mask_ = tf.tile(d_mask, mult)\n",
        "print(causal_mask_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_YgbB3zeu8x",
        "outputId": "6ee00f93-dead-45a7-8a21-4f5b5a9b3126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[1 0 0 0 0]\n",
            "  [1 1 0 0 0]\n",
            "  [1 1 1 0 0]\n",
            "  [1 1 1 1 0]\n",
            "  [1 1 1 1 1]]\n",
            "\n",
            " [[1 0 0 0 0]\n",
            "  [1 1 0 0 0]\n",
            "  [1 1 1 0 0]\n",
            "  [1 1 1 1 0]\n",
            "  [1 1 1 1 1]]], shape=(2, 5, 5), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example above:\n",
        "\n",
        "sequence length = 5\n",
        "\n",
        "batch size = 2"
      ],
      "metadata": {
        "id": "bTB56QC6gwPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know more about masking, refer [here](https://www.tensorflow.org/guide/keras/masking_and_padding)."
      ],
      "metadata": {
        "id": "YDJ5XdcUU6B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Decoder"
      ],
      "metadata": {
        "id": "jccpLKpUEwCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        # Define the layers. Let's point them out in the diagram\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        # Now we have 2 MultiHead Attention layers - one for self attention and one for generalized attention\n",
        "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation=\"relu\"),\n",
        "                                            layers.Dense(embed_dim),]\n",
        "                                           )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True   #ensures that the layer will propagate its input mask to its outputs;\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1])) # sequence_length == input_shape[1]\n",
        "        mult = tf.concat([tf.expand_dims(batch_size, -1),\n",
        "                          tf.constant([1, 1], dtype=tf.int32)],\n",
        "                         axis=0\n",
        "                         )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None): # two inputs: decoder i/p and encoder o/p\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        # print(f\"*** test: mask = {causal_mask}\")\n",
        "        padding_mask = None\n",
        "\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask) # union of 0s\n",
        "            # print(f\"**** test padding mask: {padding_mask}\")\n",
        "\n",
        "        attention_output_1 = self.attention_1(query=inputs,                     # Q: What kind of attention?  A: self attention\n",
        "                                              value=inputs,\n",
        "                                              key=inputs,\n",
        "                                              attention_mask=causal_mask        # Q: What will the causal_mask do? A: makes attention score of a query independent of future tokens\n",
        "                                              )\n",
        "\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(query=attention_output_1,         # Q: Is this self attention? A: No. This is generalised attention\n",
        "                                              value=encoder_outputs,            # Key and Value coming from encoder\n",
        "                                              key=encoder_outputs,\n",
        "                                              attention_mask=padding_mask,\n",
        "                                              )\n",
        "\n",
        "        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ],
      "metadata": {
        "id": "T92X85GUFCLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST6%20Transformer%20Network.png\" width=350px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "3Mix-Vvd9Byv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# English to spanish translation\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs) # Q: First arg acts like a 'vocabulary' for pos embedding layer\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x) #Q: What are these arguments? A: embedding dimension, no. of neurons in dense layer, no. of head in multi-head attention layer\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs) # Q: What are the call arguments in the picture?\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs) # Note that there are two input layers\n",
        "\n",
        "transformer.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "0C1QP9ubAXx4",
        "outputId": "e92ad345-1cc0-45a6-bf1f-89306510710f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ english (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ spanish (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ positional_embedding_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │      \u001b[38;5;34m3,845,120\u001b[0m │ english[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)     │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ positional_embedding_2    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │      \u001b[38;5;34m3,845,120\u001b[0m │ spanish[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)     │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │      \u001b[38;5;34m3,155,456\u001b[0m │ positional_embedding_… │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_decoder       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │      \u001b[38;5;34m5,259,520\u001b[0m │ positional_embedding_… │\n",
              "│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)      │                        │                │ transformer_encoder_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ transformer_decoder[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m15000\u001b[0m)      │      \u001b[38;5;34m3,855,000\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ english (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ spanish (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ positional_embedding_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,845,120</span> │ english[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)     │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ positional_embedding_2    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,845,120</span> │ spanish[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)     │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,155,456</span> │ positional_embedding_… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_decoder       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,259,520</span> │ positional_embedding_… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)      │                        │                │ transformer_encoder_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_decoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,855,000</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,960,216\u001b[0m (76.14 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,960,216</span> (76.14 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,960,216\u001b[0m (76.14 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,960,216</span> (76.14 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Machine Translation Example\n",
        "\n",
        "English to Spanish translation"
      ],
      "metadata": {
        "id": "zsFTQP9S6s-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the data"
      ],
      "metadata": {
        "id": "B0Y2NmDV7lF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rows of the dataset\n",
        "!tail spa-eng/spa.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxaC6lh95Ox2",
        "outputId": "87f15b35-8482-4dfc-c638-97e7a4ba3d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You can't view Flash content on an iPad. However, you can easily email yourself the URLs of these web pages and view that content on your regular computer when you get home.\tNo puedes ver contenido en Flash en un iPad. Sin embargo, puedes fácilmente enviarte por correo electrónico las URL's de esas páginas web y ver el contenido en tu computadora cuando llegas a casa.\n",
            "A mistake young people often make is to start learning too many languages at the same time, as they underestimate the difficulties and overestimate their own ability to learn them.\tUn error que cometen a menudo los jóvenes es el de comenzar a aprender demasiadas lenguas al mismo tiempo, porque subestiman sus dificultades y sobrestiman sus propias capacidades para aprenderlas.\n",
            "No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\tNo importa cuánto insistas en convencer a la gente de que el chocolate es vainilla, seguirá siendo chocolate, aunque puede que te convenzas a ti mismo y a algunos otros de que es vainilla.\n",
            "In 1969, Roger Miller recorded a song called \"You Don't Want My Love.\" Today, this song is better known as \"In the Summer Time.\" It's the first song he wrote and sang that became popular.\tEn 1969, Roger Miller grabó una canción llamada \"Tú no quieres mi amor\". Hoy, esta canción es más conocida como \"En el verano\". Es la primera canción que escribió y cantó que se convirtió popular.\n",
            "A child who is a native speaker usually knows many things about his or her language that a non-native speaker who has been studying for years still does not know and perhaps will never know.\tUn niño que es hablante nativo normalmente sabe muchas cosas acerca de su lengua que un hablante no nativo que lo haya estado estudiando durante muchos años no sabe todavía y que quizá no sabrá nunca.\n",
            "There are four main causes of alcohol-related death. Injury from car accidents or violence is one. Diseases like cirrhosis of the liver, cancer, heart and blood system diseases are the others.\tHay cuatro causas principales de muertes relacionadas con el alcohol. Lesión por un accidente automovilístico o violencia es una. Enfermedades como cirrosis del hígado, cáncer, enfermedades del corazón y del sistema circulatorio son las otras.\n",
            "There are mothers and fathers who will lie awake after the children fall asleep and wonder how they'll make the mortgage, or pay their doctor's bills, or save enough for their child's college education.\tHay madres y padres que se quedan despiertos después de que sus hijos se hayan dormido y se preguntan cómo conseguir pagar la hipoteca o las facturas del médico, o cómo ahorrar el suficiente dinero para la educación universitaria de sus hijos.\n",
            "A carbon footprint is the amount of carbon dioxide pollution that we produce as a result of our activities. Some people try to reduce their carbon footprint because they are concerned about climate change.\tUna huella de carbono es la cantidad de contaminación de dióxido de carbono que producimos como producto de nuestras actividades. Algunas personas intentan reducir su huella de carbono porque están preocupados acerca del cambio climático.\n",
            "Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Google and hope for something less irritating.\tComo suele haber varias páginas web sobre cualquier tema, normalmente sólo le doy al botón de retroceso cuando entro en una página web que tiene anuncios en ventanas emergentes. Simplemente voy a la siguiente página encontrada por Google y espero encontrar algo menos irritante.\n",
            "If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\tSi quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing: Separating input and output sequences\n",
        "text_file = \"spa-eng/spa.txt\"\n",
        "\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "\n",
        "for line in lines:\n",
        "    english, spanish = line.split(\"\\t\")\n",
        "    spanish = \"[start] \" + spanish + \" [end]\"\n",
        "    text_pairs.append((english, spanish))\n",
        "\n",
        "print(random.choice(text_pairs))\n",
        "print(f\"no. of pairs: {len(text_pairs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxxtnbv9D2-u",
        "outputId": "391104f5-e04d-4e7e-c396-76dc124e20c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Are there any questions?', '[start] ¿Hay alguna pregunta? [end]')\n",
            "no. of pairs: 118964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data\n",
        "\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ],
      "metadata": {
        "id": "mcybUiQTD3Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(string.punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj9HjcGE7h4N",
        "outputId": "9159d7af-3c63-41e3-a459-a1d977feac28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing the English and Spanish text pairs\n",
        "\n",
        "# Define which characters to strip out for spanish data- [, ], ¿\n",
        "strip_chars = string.punctuation + \"¿\"  # strip out stadard punctuations + extra one in spanish\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "# Custom standardization function for spanish\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(    # Replace elements of input matching regex pattern with rewrite.\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "id": "kBc6EItWD3C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = tf.range(10)\n",
        "dec_in = seq[:-1]\n",
        "dec_out = seq[1:]\n",
        "\n",
        "print(f\"original seq:  {seq}\")\n",
        "print(f\"dec_in:   {dec_in}\")\n",
        "print(f\"dec_out:  {dec_out}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HndSHekBRwZ",
        "outputId": "f001fb78-9a7e-4acc-dfca-5ceb9b445c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original seq:  [0 1 2 3 4 5 6 7 8 9]\n",
            "dec_in:   [0 1 2 3 4 5 6 7 8]\n",
            "dec_out:  [1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing datasets for the translation task\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "#IMPORTANT- returns nested tuple- ( (eng_encod_input, spa_ decod_input), spa_decod_output)\n",
        "def format_dataset(eng, spa):\n",
        "    # Q: What are eng and spa pre and post re-assignment ? A: raw text and indices\n",
        "    eng = source_vectorization(eng)\n",
        "    spa = target_vectorization(spa)\n",
        "    return ({\n",
        "        \"english\": eng,           # encoder input\n",
        "        \"spanish\": spa[:, :-1],    # decoder input Q: what is the first axis?  A: shape = (batch_size, )\n",
        "    }, spa[:, 1:])                  # decoder ouput\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache() #Use in-memory caching to speed up preprocessing.\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "JMicSLvGD3E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A2wVFs8Epoy",
        "outputId": "397b56d9-dd94-4304-ce22-6d883c42bb57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and evaluate the model *(Switch to GPU runtime if needed)*"
      ],
      "metadata": {
        "id": "54BdC0gg7pQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(optimizer=\"rmsprop\",\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=[\"accuracy\"]\n",
        "                    )"
      ],
      "metadata": {
        "id": "-16ZBlBkm9Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.fit(train_ds,\n",
        "                validation_data=val_ds,\n",
        "                epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XumS29R6PkRr",
        "outputId": "b4c83651-a22d-4059-a5b9-6fa37b10400d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 56ms/step - accuracy: 0.7129 - loss: 2.2122 - val_accuracy: 0.8056 - val_loss: 1.2158\n",
            "Epoch 2/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 47ms/step - accuracy: 0.8147 - loss: 1.1834 - val_accuracy: 0.8487 - val_loss: 0.8998\n",
            "Epoch 3/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 44ms/step - accuracy: 0.8517 - loss: 0.9057 - val_accuracy: 0.8669 - val_loss: 0.7860\n",
            "Epoch 4/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 44ms/step - accuracy: 0.8677 - loss: 0.7873 - val_accuracy: 0.8755 - val_loss: 0.7330\n",
            "Epoch 5/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 44ms/step - accuracy: 0.8777 - loss: 0.7146 - val_accuracy: 0.8783 - val_loss: 0.7192\n",
            "Epoch 6/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 46ms/step - accuracy: 0.8850 - loss: 0.6662 - val_accuracy: 0.8803 - val_loss: 0.7093\n",
            "Epoch 7/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 45ms/step - accuracy: 0.8907 - loss: 0.6284 - val_accuracy: 0.8822 - val_loss: 0.7074\n",
            "Epoch 8/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 44ms/step - accuracy: 0.8957 - loss: 0.5989 - val_accuracy: 0.8834 - val_loss: 0.7132\n",
            "Epoch 9/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 45ms/step - accuracy: 0.8993 - loss: 0.5773 - val_accuracy: 0.8837 - val_loss: 0.7215\n",
            "Epoch 10/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 44ms/step - accuracy: 0.9027 - loss: 0.5582 - val_accuracy: 0.8842 - val_loss: 0.7260\n",
            "Epoch 11/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 45ms/step - accuracy: 0.9053 - loss: 0.5436 - val_accuracy: 0.8840 - val_loss: 0.7364\n",
            "Epoch 12/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 47ms/step - accuracy: 0.9078 - loss: 0.5302 - val_accuracy: 0.8840 - val_loss: 0.7473\n",
            "Epoch 13/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 44ms/step - accuracy: 0.9103 - loss: 0.5173 - val_accuracy: 0.8830 - val_loss: 0.7585\n",
            "Epoch 14/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 44ms/step - accuracy: 0.9121 - loss: 0.5084 - val_accuracy: 0.8849 - val_loss: 0.7531\n",
            "Epoch 15/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 44ms/step - accuracy: 0.9143 - loss: 0.4983 - val_accuracy: 0.8830 - val_loss: 0.7705\n",
            "Epoch 16/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 44ms/step - accuracy: 0.9160 - loss: 0.4898 - val_accuracy: 0.8846 - val_loss: 0.7827\n",
            "Epoch 17/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 44ms/step - accuracy: 0.9177 - loss: 0.4805 - val_accuracy: 0.8849 - val_loss: 0.7890\n",
            "Epoch 18/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 46ms/step - accuracy: 0.9191 - loss: 0.4743 - val_accuracy: 0.8840 - val_loss: 0.7942\n",
            "Epoch 19/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 44ms/step - accuracy: 0.9207 - loss: 0.4660 - val_accuracy: 0.8832 - val_loss: 0.8130\n",
            "Epoch 20/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 45ms/step - accuracy: 0.9221 - loss: 0.4598 - val_accuracy: 0.8835 - val_loss: 0.8297\n",
            "Epoch 21/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 44ms/step - accuracy: 0.9232 - loss: 0.4541 - val_accuracy: 0.8850 - val_loss: 0.8134\n",
            "Epoch 22/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 45ms/step - accuracy: 0.9245 - loss: 0.4475 - val_accuracy: 0.8850 - val_loss: 0.8291\n",
            "Epoch 23/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 44ms/step - accuracy: 0.9253 - loss: 0.4436 - val_accuracy: 0.8841 - val_loss: 0.8441\n",
            "Epoch 24/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 45ms/step - accuracy: 0.9268 - loss: 0.4385 - val_accuracy: 0.8850 - val_loss: 0.8505\n",
            "Epoch 25/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 45ms/step - accuracy: 0.9278 - loss: 0.4325 - val_accuracy: 0.8833 - val_loss: 0.8704\n",
            "Epoch 26/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 45ms/step - accuracy: 0.9289 - loss: 0.4278 - val_accuracy: 0.8842 - val_loss: 0.8711\n",
            "Epoch 27/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 44ms/step - accuracy: 0.9298 - loss: 0.4237 - val_accuracy: 0.8843 - val_loss: 0.8773\n",
            "Epoch 28/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 44ms/step - accuracy: 0.9308 - loss: 0.4186 - val_accuracy: 0.8846 - val_loss: 0.8819\n",
            "Epoch 29/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 44ms/step - accuracy: 0.9317 - loss: 0.4150 - val_accuracy: 0.8826 - val_loss: 0.9096\n",
            "Epoch 30/30\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 44ms/step - accuracy: 0.9323 - loss: 0.4121 - val_accuracy: 0.8846 - val_loss: 0.9052\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7df40efd3d90>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save model"
      ],
      "metadata": {
        "id": "fqvG6xs42X3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.save(\"trained-transformer-model.keras\")"
      ],
      "metadata": {
        "id": "aucv-0CS1qcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "trviWIKk1XkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(4):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\"*50)\n",
        "    print(f\"Input (eng):  {input_sentence}\")\n",
        "    print(f\"Output(spa): {decode_sequence(input_sentence)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdjEkvP2oC6U",
        "outputId": "7a27e5c0-2446-4a53-a9c3-99c3b39e490b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "Input (eng):  Would you like go out for a drink after work?\n",
            "Output(spa): [start] te gustaría salir a tomar un trabajo [end]\n",
            "--------------------------------------------------\n",
            "Input (eng):  I've bought a cup of coffee.\n",
            "Output(spa): [start] compré una taza de café [end]\n",
            "--------------------------------------------------\n",
            "Input (eng):  Tom sewed the button back on his shirt.\n",
            "Output(spa): [start] tom puso un botón en su camisa [end]\n",
            "--------------------------------------------------\n",
            "Input (eng):  Has the movie started yet?\n",
            "Output(spa): [start] todavía ha empezado la película [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that both the TransformerEncoder and the TransformerDecoder are shape-invariant, so you could be stacking many of them to create a more powerful encoder or decoder."
      ],
      "metadata": {
        "id": "5ije_bIxsnGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST%206%20last%20image.png\" width=600px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "jHJf19zldfi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Technical Ungraded Questions:\n",
        "\n",
        "1. Connection between encoder outputs and decoder inputs when there are multiple stacks of them?\n",
        "\n",
        "    **Answer:** The output from the last encoder block acts as input to all decoder blocks.\n",
        "\n",
        "\\\\\n",
        "\n",
        "2. During training, are the decoder inputs obtained from decoder predictions or are they obtained directly from the target data?\n",
        "\n",
        "    **Answer:** During training, the decoder input is obtained directly from the target data. The only differnce between the decoder input and decoder target is an offset of 1 index. For example, consider a hindi to english translation problem with a an english sample \"[start] I like to learn [end]\".  The input to the decoder for this sample will be sample[:-1], i.e. \"[start] I like to learn\" and the target will be sample[1:], i.e. \"I like to learn [end]\". The prediction during training will be a probabilitly distribution over the vocabulary for each element in the sequence. So if the sequence length is 8 and the vocabulary size is 100, then the output shape of the prediction for the given sample will be (6,100). The actual predicted sequence can be computed by taking the argmax, i.e. the token with the maximum probability, for each token in the sequence. An exemplary prediction based on our example can be \"I love to study\". The loss will be computed based on the sum of cross-entropy losses for each token. Here 'like'/'love' and 'learn'/'study' will contribute to the loss.\n",
        "\n",
        "  (Notes:\n",
        "  1. The sample will actually have integer data. Here its written text for the sake of clarity\n",
        "  2. The above explanation is for 1 sample. If the batch size of 64, i.e. 64 samples in a mini-batch , then the decoder output shape is (64,6,100). In general, it is (batch_size, seq_length, vocab_size).\n"
      ],
      "metadata": {
        "id": "xce42Pfp-9vM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other important points:\n",
        "- An advatage of Transformers (over RNNs) is that they allow parallizable computations. Note that the computation of given token does not depend on the computations of the previous token, and can be done in parallel during training.\n",
        "\n",
        "- Note what kind of data structure the the function \"format_dataset(eng, spa)\" returns. It is a nested tuple- ( (eng_encod_input, spa_decod_input), spa_decod_output), where '(eng_encod_input, spa_decod_input)' form the input of the Transformer Model and 'spa_decod_output' is the target output of the Transfomrmer Model."
      ],
      "metadata": {
        "id": "_oX_gVNGnjbI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErjQyyi4nR2n"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "#@title In the transformer decoder, why is masking applied to the self-attention layer? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"To prevent information leakage from future tokens in the sequence\", \"To increase computational efficiency\", \"To enhance the learning of positional encodings\", \"To align the output sequence with the input sequence\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}